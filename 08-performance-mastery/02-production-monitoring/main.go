/*
=== Goæ€§èƒ½æŒæ§ï¼šç”Ÿäº§ç¯å¢ƒç›‘æ§ç³»ç»Ÿ ===

æœ¬æ¨¡å—ä¸“æ³¨äºæ„å»ºä¼ä¸šçº§ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ç›‘æ§ä½“ç³»ï¼Œæ¢ç´¢ï¼š
1. APM (Application Performance Monitoring) ç³»ç»Ÿæ¶æ„
2. åˆ†å¸ƒå¼è¿½è¸ªå’Œé“¾è·¯ç›‘æ§
3. æœåŠ¡å¥åº·æ£€æŸ¥å’Œå¯ç”¨æ€§ç›‘æ§
4. SLI/SLO (Service Level Indicators/Objectives) ç®¡ç†
5. å®æ—¶å‘Šè­¦å’Œé€šçŸ¥ç³»ç»Ÿ
6. æ€§èƒ½ä»ªè¡¨æ¿å’Œå¯è§†åŒ–
7. è‡ªåŠ¨æ‰©ç¼©å®¹å’Œè´Ÿè½½å‡è¡¡
8. æ•…éšœæ£€æµ‹å’Œè‡ªåŠ¨æ¢å¤
9. å®¹é‡è§„åˆ’å’Œé¢„æµ‹
10. æ€§èƒ½ä¼˜åŒ–å»ºè®®å¼•æ“

å­¦ä¹ ç›®æ ‡ï¼š
- æ„å»ºå®Œæ•´çš„ç”Ÿäº§ç¯å¢ƒç›‘æ§ä½“ç³»
- æŒæ¡å¾®æœåŠ¡æ¶æ„ä¸‹çš„æ€§èƒ½ç›‘æ§
- å®ç°æ™ºèƒ½åŒ–çš„è¿ç»´ç›‘æ§ç³»ç»Ÿ
- å­¦ä¼šå¤§è§„æ¨¡ç³»ç»Ÿçš„æ€§èƒ½ç®¡ç†
*/

package main

import (
	"context"
	"crypto/rand"
	"fmt"
	"math/big"
	"sort"
	"strings"
	"sync"
	"sync/atomic"
	"time"
)

// å®‰å…¨éšæœºæ•°ç”Ÿæˆå‡½æ•°
func secureRandomInt(max int) int {
	n, err := rand.Int(rand.Reader, big.NewInt(int64(max)))
	if err != nil {
		// G115å®‰å…¨ä¿®å¤ï¼šç¡®ä¿è½¬æ¢ä¸ä¼šæº¢å‡º
		fallback := time.Now().UnixNano() % int64(max)
		if fallback > int64(^uint(0)>>1) {
			fallback = fallback % int64(^uint(0)>>1)
		}
		return int(fallback)
	}
	// G115å®‰å…¨ä¿®å¤ï¼šæ£€æŸ¥int64åˆ°intçš„å®‰å…¨è½¬æ¢
	result := n.Int64()
	if result > int64(^uint(0)>>1) {
		result = result % int64(max)
	}
	return int(result)
}

func secureRandomFloat64() float64 {
	n, err := rand.Int(rand.Reader, big.NewInt(1<<53))
	if err != nil {
		// å®‰å…¨fallbackï¼šä½¿ç”¨æ—¶é—´æˆ³
		return float64(time.Now().UnixNano()%1000) / 1000.0
	}
	return float64(n.Int64()) / float64(1<<53)
}

// ==================
// 1. APMç³»ç»Ÿæ ¸å¿ƒæ¶æ„
// ==================

// APMSystem åº”ç”¨æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
type APMSystem struct {
	services      map[string]*ServiceMonitor
	traces        *TraceCollector
	metrics       *MetricsAggregator
	alerts        *AlertManager
	dashboards    map[string]*Dashboard
	sloManager    *SLOManager
	healthChecker *HealthChecker
	config        APMConfig
	mutex         sync.RWMutex
	running       bool
	stopCh        chan struct{}
}

// APMConfig APMé…ç½®
type APMConfig struct {
	SamplingRate     float64
	MetricsInterval  time.Duration
	AlertInterval    time.Duration
	TraceRetention   time.Duration
	MetricsRetention time.Duration
	MaxTraces        int
	MaxMetrics       int
}

// ServiceMonitor æœåŠ¡ç›‘æ§å™¨
type ServiceMonitor struct {
	ServiceName   string
	Version       string
	Instance      string
	StartTime     time.Time
	RequestCount  int64
	ErrorCount    int64
	ResponseTimes []time.Duration
	Endpoints     map[string]*EndpointMetrics
	Dependencies  []string
	Health        HealthStatus
	SLI           map[string]float64
	mutex         sync.RWMutex
}

// EndpointMetrics ç«¯ç‚¹æŒ‡æ ‡
type EndpointMetrics struct {
	Path          string
	Method        string
	RequestCount  int64
	ErrorCount    int64
	ResponseTimes ResponseTimeStats
	StatusCodes   map[int]int64
	LastAccess    time.Time
}

// ResponseTimeStats å“åº”æ—¶é—´ç»Ÿè®¡
type ResponseTimeStats struct {
	P50  time.Duration
	P90  time.Duration
	P95  time.Duration
	P99  time.Duration
	Mean time.Duration
	Max  time.Duration
	Min  time.Duration
}

// HealthStatus å¥åº·çŠ¶æ€
type HealthStatus struct {
	Status     string // "healthy", "degraded", "unhealthy"
	LastCheck  time.Time
	CheckCount int64
	FailCount  int64
	Message    string
	Details    map[string]interface{}
}

// Dashboard ä»ªè¡¨æ¿
type Dashboard struct {
	ID          string
	Name        string
	Description string
	Widgets     []DashboardWidget
	CreatedAt   time.Time
	UpdatedAt   time.Time
}

// DashboardWidget ä»ªè¡¨æ¿ç»„ä»¶
type DashboardWidget struct {
	ID     string
	Type   string
	Title  string
	Config map[string]interface{}
}

func NewAPMSystem(config APMConfig) *APMSystem {
	return &APMSystem{
		services:      make(map[string]*ServiceMonitor),
		traces:        NewTraceCollector(),
		metrics:       NewMetricsAggregator(),
		alerts:        NewAlertManager(),
		dashboards:    make(map[string]*Dashboard),
		sloManager:    NewSLOManager(),
		healthChecker: NewHealthChecker(),
		config:        config,
		stopCh:        make(chan struct{}),
	}
}

func (apm *APMSystem) Start() error {
	apm.mutex.Lock()
	defer apm.mutex.Unlock()

	if apm.running {
		return fmt.Errorf("APM system already running")
	}

	apm.running = true

	// å¯åŠ¨å„ä¸ªç»„ä»¶
	go apm.metricsCollectionLoop()
	go apm.healthCheckLoop()
	go apm.alertProcessingLoop()
	go apm.traceProcessingLoop()

	fmt.Println("APMç³»ç»Ÿå·²å¯åŠ¨")
	return nil
}

func (apm *APMSystem) Stop() {
	apm.mutex.Lock()
	defer apm.mutex.Unlock()

	if !apm.running {
		return
	}

	apm.running = false
	close(apm.stopCh)
	fmt.Println("APMç³»ç»Ÿå·²åœæ­¢")
}

func (apm *APMSystem) RegisterService(serviceName, version, instance string) *ServiceMonitor {
	apm.mutex.Lock()
	defer apm.mutex.Unlock()

	key := fmt.Sprintf("%s:%s:%s", serviceName, version, instance)
	monitor := &ServiceMonitor{
		ServiceName:   serviceName,
		Version:       version,
		Instance:      instance,
		StartTime:     time.Now(),
		ResponseTimes: make([]time.Duration, 0),
		Endpoints:     make(map[string]*EndpointMetrics),
		Dependencies:  make([]string, 0),
		SLI:           make(map[string]float64),
		Health: HealthStatus{
			Status:    "healthy",
			LastCheck: time.Now(),
		},
	}

	apm.services[key] = monitor
	fmt.Printf("æ³¨å†ŒæœåŠ¡: %s\n", key)
	return monitor
}

func (apm *APMSystem) metricsCollectionLoop() {
	ticker := time.NewTicker(apm.config.MetricsInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			apm.collectMetrics()
		case <-apm.stopCh:
			return
		}
	}
}

func (apm *APMSystem) collectMetrics() {
	apm.mutex.RLock()
	services := make([]*ServiceMonitor, 0, len(apm.services))
	for _, service := range apm.services {
		services = append(services, service)
	}
	apm.mutex.RUnlock()

	for _, service := range services {
		apm.collectServiceMetrics(service)
	}
}

func (apm *APMSystem) collectServiceMetrics(service *ServiceMonitor) {
	service.mutex.Lock()
	defer service.mutex.Unlock()

	// è®¡ç®—é”™è¯¯ç‡
	if service.RequestCount > 0 {
		errorRate := float64(service.ErrorCount) / float64(service.RequestCount) * 100
		service.SLI["error_rate"] = errorRate
	}

	// è®¡ç®—å¹³å‡å“åº”æ—¶é—´
	if len(service.ResponseTimes) > 0 {
		sum := time.Duration(0)
		for _, rt := range service.ResponseTimes {
			sum += rt
		}
		avgResponseTime := sum / time.Duration(len(service.ResponseTimes))
		service.SLI["avg_response_time"] = float64(avgResponseTime.Milliseconds())
	}

	// è®¡ç®—å¯ç”¨æ€§
	uptime := time.Since(service.StartTime)
	service.SLI["uptime_hours"] = uptime.Hours()

	// è®°å½•åˆ°æŒ‡æ ‡èšåˆå™¨
	apm.metrics.RecordGauge(
		fmt.Sprintf("%s_error_rate", service.ServiceName),
		service.SLI["error_rate"],
		map[string]string{
			"service":  service.ServiceName,
			"version":  service.Version,
			"instance": service.Instance,
		},
	)

	apm.metrics.RecordGauge(
		fmt.Sprintf("%s_response_time", service.ServiceName),
		service.SLI["avg_response_time"],
		map[string]string{
			"service": service.ServiceName,
			"version": service.Version,
		},
	)
}

// ==================
// 2. åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ
// ==================

// TraceCollector è¿½è¸ªæ”¶é›†å™¨
type TraceCollector struct {
	traces    map[string]*Trace
	spans     map[string]*Span
	mutex     sync.RWMutex
	maxTraces int
}

// Trace åˆ†å¸ƒå¼è¿½è¸ª
type Trace struct {
	TraceID    string
	RootSpan   *Span
	Spans      []*Span
	Services   map[string]bool
	StartTime  time.Time
	EndTime    time.Time
	Duration   time.Duration
	Status     TraceStatus
	ErrorCount int
	SpanCount  int
}

// Span è¿½è¸ªæ®µ
type Span struct {
	SpanID        string
	TraceID       string
	ParentSpanID  string
	OperationName string
	ServiceName   string
	StartTime     time.Time
	EndTime       time.Time
	Duration      time.Duration
	Tags          map[string]string
	Logs          []SpanLog
	Status        SpanStatus
	Error         error
}

// SpanLog æ®µæ—¥å¿—
type SpanLog struct {
	Timestamp time.Time
	Level     string
	Message   string
	Fields    map[string]interface{}
}

// TraceStatus è¿½è¸ªçŠ¶æ€
type TraceStatus int

const (
	TraceStatusOK TraceStatus = iota
	TraceStatusError
	TraceStatusTimeout
)

// SpanStatus æ®µçŠ¶æ€
type SpanStatus int

const (
	SpanStatusOK SpanStatus = iota
	SpanStatusError
)

func NewTraceCollector() *TraceCollector {
	return &TraceCollector{
		traces:    make(map[string]*Trace),
		spans:     make(map[string]*Span),
		maxTraces: 10000,
	}
}

func (tc *TraceCollector) StartTrace(serviceName, operationName string) *Trace {
	traceID := tc.generateID()
	rootSpanID := tc.generateID()

	rootSpan := &Span{
		SpanID:        rootSpanID,
		TraceID:       traceID,
		OperationName: operationName,
		ServiceName:   serviceName,
		StartTime:     time.Now(),
		Tags:          make(map[string]string),
		Logs:          make([]SpanLog, 0),
		Status:        SpanStatusOK,
	}

	trace := &Trace{
		TraceID:   traceID,
		RootSpan:  rootSpan,
		Spans:     []*Span{rootSpan},
		Services:  map[string]bool{serviceName: true},
		StartTime: time.Now(),
		Status:    TraceStatusOK,
	}

	tc.mutex.Lock()
	tc.traces[traceID] = trace
	tc.spans[rootSpanID] = rootSpan

	// é™åˆ¶è¿½è¸ªæ•°é‡
	if len(tc.traces) > tc.maxTraces {
		tc.cleanupOldTraces()
	}
	tc.mutex.Unlock()

	return trace
}

func (tc *TraceCollector) StartSpan(traceID, parentSpanID, serviceName, operationName string) *Span {
	spanID := tc.generateID()

	span := &Span{
		SpanID:        spanID,
		TraceID:       traceID,
		ParentSpanID:  parentSpanID,
		OperationName: operationName,
		ServiceName:   serviceName,
		StartTime:     time.Now(),
		Tags:          make(map[string]string),
		Logs:          make([]SpanLog, 0),
		Status:        SpanStatusOK,
	}

	tc.mutex.Lock()
	defer tc.mutex.Unlock()

	tc.spans[spanID] = span

	if trace, exists := tc.traces[traceID]; exists {
		trace.Spans = append(trace.Spans, span)
		trace.Services[serviceName] = true
		trace.SpanCount++
	}

	return span
}

func (tc *TraceCollector) FinishSpan(spanID string) {
	tc.mutex.Lock()
	defer tc.mutex.Unlock()

	if span, exists := tc.spans[spanID]; exists {
		span.EndTime = time.Now()
		span.Duration = span.EndTime.Sub(span.StartTime)

		// å¦‚æœæ˜¯æ ¹spanï¼Œæ›´æ–°trace
		if trace, exists := tc.traces[span.TraceID]; exists && trace.RootSpan.SpanID == spanID {
			trace.EndTime = span.EndTime
			trace.Duration = span.Duration
		}
	}
}

func (tc *TraceCollector) generateID() string {
	bytes := make([]byte, 16)
	rand.Read(bytes)
	return fmt.Sprintf("%x", bytes)
}

func (tc *TraceCollector) cleanupOldTraces() {
	// æ¸…ç†æœ€è€çš„25%è¿½è¸ª
	traces := make([]*Trace, 0, len(tc.traces))
	for _, trace := range tc.traces {
		traces = append(traces, trace)
	}

	sort.Slice(traces, func(i, j int) bool {
		return traces[i].StartTime.Before(traces[j].StartTime)
	})

	cleanupCount := len(traces) / 4
	for i := 0; i < cleanupCount; i++ {
		trace := traces[i]
		delete(tc.traces, trace.TraceID)
		for _, span := range trace.Spans {
			delete(tc.spans, span.SpanID)
		}
	}
}

func (tc *TraceCollector) GetTrace(traceID string) *Trace {
	tc.mutex.RLock()
	defer tc.mutex.RUnlock()

	return tc.traces[traceID]
}

func (tc *TraceCollector) GetTraceStats() map[string]interface{} {
	tc.mutex.RLock()
	defer tc.mutex.RUnlock()

	serviceCount := make(map[string]int)
	var totalDuration time.Duration
	errorCount := 0

	for _, trace := range tc.traces {
		for service := range trace.Services {
			serviceCount[service]++
		}
		totalDuration += trace.Duration
		if trace.Status == TraceStatusError {
			errorCount++
		}
	}

	avgDuration := time.Duration(0)
	if len(tc.traces) > 0 {
		avgDuration = totalDuration / time.Duration(len(tc.traces))
	}

	return map[string]interface{}{
		"total_traces":    len(tc.traces),
		"total_spans":     len(tc.spans),
		"error_traces":    errorCount,
		"avg_duration_ms": avgDuration.Milliseconds(),
		"services":        serviceCount,
	}
}

// ==================
// 3. æŒ‡æ ‡èšåˆå™¨
// ==================

// MetricsAggregator æŒ‡æ ‡èšåˆå™¨
type MetricsAggregator struct {
	metrics    map[string]*AggregatedMetric
	timeSeries map[string][]TimeSeriesPoint
	mutex      sync.RWMutex
	maxMetrics int
}

// AggregatedMetric èšåˆæŒ‡æ ‡
type AggregatedMetric struct {
	Name        string
	Type        string
	Value       float64
	Count       int64
	Sum         float64
	Min         float64
	Max         float64
	Labels      map[string]string
	LastUpdated time.Time
	Histogram   *HistogramData
}

// TimeSeriesPoint æ—¶é—´åºåˆ—ç‚¹
type TimeSeriesPoint struct {
	Timestamp time.Time
	Value     float64
}

// HistogramData ç›´æ–¹å›¾æ•°æ®
type HistogramData struct {
	Buckets []HistogramBucket
	Count   int64
	Sum     float64
}

// HistogramBucket ç›´æ–¹å›¾æ¡¶
type HistogramBucket struct {
	UpperBound float64
	Count      int64
}

func NewMetricsAggregator() *MetricsAggregator {
	return &MetricsAggregator{
		metrics:    make(map[string]*AggregatedMetric),
		timeSeries: make(map[string][]TimeSeriesPoint),
		maxMetrics: 50000,
	}
}

func (ma *MetricsAggregator) RecordCounter(name string, value float64, labels map[string]string) {
	ma.mutex.Lock()
	defer ma.mutex.Unlock()

	key := ma.getMetricKey(name, labels)
	metric, exists := ma.metrics[key]
	if !exists {
		metric = &AggregatedMetric{
			Name:   name,
			Type:   "counter",
			Labels: labels,
			Min:    value,
			Max:    value,
		}
		ma.metrics[key] = metric
	}

	metric.Count++
	metric.Sum += value
	metric.Value = metric.Sum
	metric.LastUpdated = time.Now()

	if value < metric.Min {
		metric.Min = value
	}
	if value > metric.Max {
		metric.Max = value
	}

	// æ·»åŠ åˆ°æ—¶é—´åºåˆ—
	ma.addToTimeSeries(key, metric.Value)
}

func (ma *MetricsAggregator) RecordGauge(name string, value float64, labels map[string]string) {
	ma.mutex.Lock()
	defer ma.mutex.Unlock()

	key := ma.getMetricKey(name, labels)
	metric, exists := ma.metrics[key]
	if !exists {
		metric = &AggregatedMetric{
			Name:   name,
			Type:   "gauge",
			Labels: labels,
			Min:    value,
			Max:    value,
		}
		ma.metrics[key] = metric
	}

	metric.Value = value
	metric.Count++
	metric.Sum += value
	metric.LastUpdated = time.Now()

	if value < metric.Min {
		metric.Min = value
	}
	if value > metric.Max {
		metric.Max = value
	}

	ma.addToTimeSeries(key, value)
}

func (ma *MetricsAggregator) RecordHistogram(name string, value float64, labels map[string]string, buckets []float64) {
	ma.mutex.Lock()
	defer ma.mutex.Unlock()

	key := ma.getMetricKey(name, labels)
	metric, exists := ma.metrics[key]
	if !exists {
		histBuckets := make([]HistogramBucket, len(buckets))
		for i, bound := range buckets {
			histBuckets[i] = HistogramBucket{UpperBound: bound}
		}

		metric = &AggregatedMetric{
			Name:   name,
			Type:   "histogram",
			Labels: labels,
			Min:    value,
			Max:    value,
			Histogram: &HistogramData{
				Buckets: histBuckets,
			},
		}
		ma.metrics[key] = metric
	}

	metric.Count++
	metric.Sum += value
	metric.LastUpdated = time.Now()
	metric.Histogram.Count++
	metric.Histogram.Sum += value

	if value < metric.Min {
		metric.Min = value
	}
	if value > metric.Max {
		metric.Max = value
	}

	// æ›´æ–°ç›´æ–¹å›¾æ¡¶
	for i := range metric.Histogram.Buckets {
		if value <= metric.Histogram.Buckets[i].UpperBound {
			metric.Histogram.Buckets[i].Count++
		}
	}

	ma.addToTimeSeries(key, value)
}

func (ma *MetricsAggregator) getMetricKey(name string, labels map[string]string) string {
	var parts []string
	parts = append(parts, name)

	var keys []string
	for k := range labels {
		keys = append(keys, k)
	}
	sort.Strings(keys)

	for _, k := range keys {
		parts = append(parts, fmt.Sprintf("%s=%s", k, labels[k]))
	}

	return strings.Join(parts, "|")
}

func (ma *MetricsAggregator) addToTimeSeries(key string, value float64) {
	series, exists := ma.timeSeries[key]
	if !exists {
		series = make([]TimeSeriesPoint, 0)
	}

	series = append(series, TimeSeriesPoint{
		Timestamp: time.Now(),
		Value:     value,
	})

	// ä¿æŒæœ€è¿‘1000ä¸ªç‚¹
	if len(series) > 1000 {
		series = series[100:]
	}

	ma.timeSeries[key] = series
}

func (ma *MetricsAggregator) GetMetrics() map[string]*AggregatedMetric {
	ma.mutex.RLock()
	defer ma.mutex.RUnlock()

	result := make(map[string]*AggregatedMetric)
	for k, v := range ma.metrics {
		result[k] = v
	}

	return result
}

func (ma *MetricsAggregator) Query(name string, labels map[string]string, startTime, endTime time.Time) []TimeSeriesPoint {
	ma.mutex.RLock()
	defer ma.mutex.RUnlock()

	key := ma.getMetricKey(name, labels)
	series, exists := ma.timeSeries[key]
	if !exists {
		return nil
	}

	var result []TimeSeriesPoint
	for _, point := range series {
		if point.Timestamp.After(startTime) && point.Timestamp.Before(endTime) {
			result = append(result, point)
		}
	}

	return result
}

// ==================
// 4. å‘Šè­¦ç®¡ç†ç³»ç»Ÿ
// ==================

// AlertManager å‘Šè­¦ç®¡ç†å™¨
type AlertManager struct {
	rules      []AlertRule
	alerts     []Alert
	channels   map[string]NotificationChannel
	silences   []AlertSilence
	mutex      sync.RWMutex
	evaluation time.Duration
}

// AlertRule å‘Šè­¦è§„åˆ™
type AlertRule struct {
	ID          string
	Name        string
	Query       string
	Condition   AlertCondition
	Threshold   float64
	Duration    time.Duration
	Severity    AlertSeverity
	Labels      map[string]string
	Annotations map[string]string
	Enabled     bool
}

// AlertCondition å‘Šè­¦æ¡ä»¶
type AlertCondition int

const (
	GreaterThan AlertCondition = iota
	LessThan
	Equal
	NotEqual
)

// Alert å‘Šè­¦
type Alert struct {
	ID          string
	RuleID      string
	Name        string
	Status      AlertStatus
	StartsAt    time.Time
	EndsAt      *time.Time
	Value       float64
	Threshold   float64
	Labels      map[string]string
	Annotations map[string]string
	Severity    AlertSeverity
}

// AlertStatus å‘Šè­¦çŠ¶æ€
type AlertStatus int

const (
	AlertStatusFiring AlertStatus = iota
	AlertStatusResolved
	AlertStatusSilenced
)

// AlertSeverity å‘Šè­¦ä¸¥é‡ç¨‹åº¦
type AlertSeverity int

const (
	SeverityInfo AlertSeverity = iota
	SeverityWarning
	SeverityCritical
)

// NotificationChannel é€šçŸ¥æ¸ é“
type NotificationChannel interface {
	Send(alert Alert) error
	Name() string
}

// AlertSilence å‘Šè­¦é™é»˜
type AlertSilence struct {
	ID       string
	Matchers []AlertMatcher
	StartsAt time.Time
	EndsAt   time.Time
	Comment  string
}

// AlertMatcher å‘Šè­¦åŒ¹é…å™¨
type AlertMatcher struct {
	Name    string
	Value   string
	IsRegex bool
}

func NewAlertManager() *AlertManager {
	return &AlertManager{
		rules:      make([]AlertRule, 0),
		alerts:     make([]Alert, 0),
		channels:   make(map[string]NotificationChannel),
		silences:   make([]AlertSilence, 0),
		evaluation: 30 * time.Second,
	}
}

func (am *AlertManager) AddRule(rule AlertRule) {
	am.mutex.Lock()
	defer am.mutex.Unlock()

	am.rules = append(am.rules, rule)
}

func (am *AlertManager) RegisterChannel(name string, channel NotificationChannel) {
	am.mutex.Lock()
	defer am.mutex.Unlock()

	am.channels[name] = channel
}

func (am *AlertManager) EvaluateRules(metrics map[string]*AggregatedMetric) {
	am.mutex.Lock()
	defer am.mutex.Unlock()

	now := time.Now()

	for _, rule := range am.rules {
		if !rule.Enabled {
			continue
		}

		triggered := am.evaluateRule(rule, metrics)
		existingAlert := am.findAlert(rule.ID)

		if triggered && existingAlert == nil {
			// åˆ›å»ºæ–°å‘Šè­¦
			alert := Alert{
				ID:          am.generateAlertID(),
				RuleID:      rule.ID,
				Name:        rule.Name,
				Status:      AlertStatusFiring,
				StartsAt:    now,
				Labels:      rule.Labels,
				Annotations: rule.Annotations,
				Severity:    rule.Severity,
			}

			am.alerts = append(am.alerts, alert)
			am.sendNotification(alert)
		} else if !triggered && existingAlert != nil && existingAlert.Status == AlertStatusFiring {
			// è§£å†³å‘Šè­¦
			existingAlert.Status = AlertStatusResolved
			endTime := now
			existingAlert.EndsAt = &endTime
			am.sendNotification(*existingAlert)
		}
	}
}

func (am *AlertManager) evaluateRule(rule AlertRule, metrics map[string]*AggregatedMetric) bool {
	// ç®€åŒ–çš„è§„åˆ™è¯„ä¼° - å®é™…åº”ç”¨éœ€è¦æ›´å¤æ‚çš„æŸ¥è¯¢å¼•æ“
	for _, metric := range metrics {
		// æ£€æŸ¥æ ‡ç­¾åŒ¹é…
		if am.labelsMatch(rule.Labels, metric.Labels) {
			switch rule.Condition {
			case GreaterThan:
				return metric.Value > rule.Threshold
			case LessThan:
				return metric.Value < rule.Threshold
			case Equal:
				return metric.Value == rule.Threshold
			case NotEqual:
				return metric.Value != rule.Threshold
			}
		}
	}

	return false
}

func (am *AlertManager) labelsMatch(ruleLabels, metricLabels map[string]string) bool {
	for key, value := range ruleLabels {
		if metricValue, exists := metricLabels[key]; !exists || metricValue != value {
			return false
		}
	}
	return true
}

func (am *AlertManager) findAlert(ruleID string) *Alert {
	for i := range am.alerts {
		if am.alerts[i].RuleID == ruleID && am.alerts[i].Status == AlertStatusFiring {
			return &am.alerts[i]
		}
	}
	return nil
}

func (am *AlertManager) sendNotification(alert Alert) {
	for _, channel := range am.channels {
		go func(ch NotificationChannel) {
			if err := ch.Send(alert); err != nil {
				fmt.Printf("å‘é€å‘Šè­¦é€šçŸ¥å¤±è´¥: %v\n", err)
			}
		}(channel)
	}
}

func (am *AlertManager) generateAlertID() string {
	return fmt.Sprintf("alert_%d", time.Now().UnixNano())
}

func (am *AlertManager) GetActiveAlerts() []Alert {
	am.mutex.RLock()
	defer am.mutex.RUnlock()

	var active []Alert
	for _, alert := range am.alerts {
		if alert.Status == AlertStatusFiring {
			active = append(active, alert)
		}
	}

	return active
}

// ==================
// 5. SLOç®¡ç†ç³»ç»Ÿ
// ==================

// SLOManager SLOç®¡ç†å™¨
type SLOManager struct {
	slos         map[string]*SLO
	slis         map[string]*SLI
	errorBudgets map[string]*ErrorBudget
	mutex        sync.RWMutex
}

// SLO æœåŠ¡ç­‰çº§ç›®æ ‡
type SLO struct {
	Name        string
	Description string
	Target      float64 // ç™¾åˆ†æ¯”ï¼Œå¦‚99.9
	TimeWindow  time.Duration
	SLIQuery    string
	AlertRules  []string
	Labels      map[string]string
}

// SLI æœåŠ¡ç­‰çº§æŒ‡æ ‡
type SLI struct {
	Name        string
	Type        SLIType
	GoodEvents  int64
	TotalEvents int64
	Value       float64
	LastUpdated time.Time
	History     []SLIDataPoint
}

// SLIType SLIç±»å‹
type SLIType int

const (
	SLIAvailability SLIType = iota
	SLILatency
	SLIThroughput
	SLIErrorRate
)

// SLIDataPoint SLIæ•°æ®ç‚¹
type SLIDataPoint struct {
	Timestamp time.Time
	Value     float64
}

// ErrorBudget é”™è¯¯é¢„ç®—
type ErrorBudget struct {
	SLOName         string
	TotalBudget     float64
	ConsumedBudget  float64
	RemainingBudget float64
	BurnRate        float64
	TimeWindow      time.Duration
	LastCalculated  time.Time
}

func NewSLOManager() *SLOManager {
	return &SLOManager{
		slos:         make(map[string]*SLO),
		slis:         make(map[string]*SLI),
		errorBudgets: make(map[string]*ErrorBudget),
	}
}

func (sm *SLOManager) DefineSLO(slo SLO) {
	sm.mutex.Lock()
	defer sm.mutex.Unlock()

	sm.slos[slo.Name] = &slo

	// åˆå§‹åŒ–é”™è¯¯é¢„ç®—
	errorBudget := &ErrorBudget{
		SLOName:         slo.Name,
		TotalBudget:     100 - slo.Target,
		ConsumedBudget:  0,
		RemainingBudget: 100 - slo.Target,
		TimeWindow:      slo.TimeWindow,
		LastCalculated:  time.Now(),
	}

	sm.errorBudgets[slo.Name] = errorBudget
}

func (sm *SLOManager) UpdateSLI(name string, sliType SLIType, goodEvents, totalEvents int64) {
	sm.mutex.Lock()
	defer sm.mutex.Unlock()

	sli, exists := sm.slis[name]
	if !exists {
		sli = &SLI{
			Name:    name,
			Type:    sliType,
			History: make([]SLIDataPoint, 0),
		}
		sm.slis[name] = sli
	}

	sli.GoodEvents = goodEvents
	sli.TotalEvents = totalEvents
	if totalEvents > 0 {
		sli.Value = float64(goodEvents) / float64(totalEvents) * 100
	}
	sli.LastUpdated = time.Now()

	// æ·»åŠ åˆ°å†å²
	sli.History = append(sli.History, SLIDataPoint{
		Timestamp: time.Now(),
		Value:     sli.Value,
	})

	// ä¿æŒæœ€è¿‘1000ä¸ªæ•°æ®ç‚¹
	if len(sli.History) > 1000 {
		sli.History = sli.History[100:]
	}
}

func (sm *SLOManager) CalculateErrorBudgets() {
	sm.mutex.Lock()
	defer sm.mutex.Unlock()

	for sloName, slo := range sm.slos {
		errorBudget := sm.errorBudgets[sloName]
		if errorBudget == nil {
			continue
		}

		// æŸ¥æ‰¾å¯¹åº”çš„SLI
		sli, exists := sm.slis[sloName]
		if !exists {
			continue
		}

		// è®¡ç®—æ—¶é—´çª—å£å†…çš„é”™è¯¯é¢„ç®—æ¶ˆè€—
		now := time.Now()
		windowStart := now.Add(-slo.TimeWindow)

		var totalEvents, goodEvents int64
		for _, point := range sli.History {
			if point.Timestamp.After(windowStart) {
				totalEvents++
				if point.Value >= slo.Target {
					goodEvents++
				}
			}
		}

		if totalEvents > 0 {
			actualSLI := float64(goodEvents) / float64(totalEvents) * 100
			consumedBudget := slo.Target - actualSLI
			if consumedBudget < 0 {
				consumedBudget = 0
			}

			errorBudget.ConsumedBudget = consumedBudget
			errorBudget.RemainingBudget = errorBudget.TotalBudget - consumedBudget

			// è®¡ç®—ç‡ƒçƒ§ç‡ï¼ˆç®€åŒ–ï¼‰
			if errorBudget.TotalBudget > 0 {
				errorBudget.BurnRate = consumedBudget / errorBudget.TotalBudget
			}
		}

		errorBudget.LastCalculated = now
	}
}

func (sm *SLOManager) GetSLOStatus() map[string]SLOStatus {
	sm.mutex.RLock()
	defer sm.mutex.RUnlock()

	status := make(map[string]SLOStatus)

	for name, slo := range sm.slos {
		sli := sm.slis[name]
		errorBudget := sm.errorBudgets[name]

		sloStatus := SLOStatus{
			Name:   name,
			Target: slo.Target,
		}

		if sli != nil {
			sloStatus.CurrentSLI = sli.Value
			sloStatus.Compliant = sli.Value >= slo.Target
		}

		if errorBudget != nil {
			sloStatus.ErrorBudget = *errorBudget
		}

		status[name] = sloStatus
	}

	return status
}

// SLOStatus SLOçŠ¶æ€
type SLOStatus struct {
	Name        string
	Target      float64
	CurrentSLI  float64
	Compliant   bool
	ErrorBudget ErrorBudget
}

// ==================
// 6. å¥åº·æ£€æŸ¥ç³»ç»Ÿ
// ==================

// HealthChecker å¥åº·æ£€æŸ¥å™¨
type HealthChecker struct {
	checks   map[string]HealthCheck
	results  map[string]HealthCheckResult
	mutex    sync.RWMutex
	interval time.Duration
}

// HealthCheck å¥åº·æ£€æŸ¥
type HealthCheck interface {
	Name() string
	Check(ctx context.Context) HealthCheckResult
	Timeout() time.Duration
}

// HealthCheckResult å¥åº·æ£€æŸ¥ç»“æœ
type HealthCheckResult struct {
	Name      string
	Status    HealthCheckStatus
	Message   string
	Duration  time.Duration
	Timestamp time.Time
	Details   map[string]interface{}
}

// HealthCheckStatus å¥åº·æ£€æŸ¥çŠ¶æ€
type HealthCheckStatus int

const (
	HealthStatusUp HealthCheckStatus = iota
	HealthStatusDown
	HealthStatusDegraded
)

func NewHealthChecker() *HealthChecker {
	return &HealthChecker{
		checks:   make(map[string]HealthCheck),
		results:  make(map[string]HealthCheckResult),
		interval: 30 * time.Second,
	}
}

func (hc *HealthChecker) RegisterCheck(check HealthCheck) {
	hc.mutex.Lock()
	defer hc.mutex.Unlock()

	hc.checks[check.Name()] = check
}

func (hc *HealthChecker) RunChecks(ctx context.Context) {
	hc.mutex.RLock()
	checks := make([]HealthCheck, 0, len(hc.checks))
	for _, check := range hc.checks {
		checks = append(checks, check)
	}
	hc.mutex.RUnlock()

	var wg sync.WaitGroup
	for _, check := range checks {
		wg.Add(1)
		go func(c HealthCheck) {
			defer wg.Done()

			checkCtx, cancel := context.WithTimeout(ctx, c.Timeout())
			defer cancel()

			start := time.Now()
			result := c.Check(checkCtx)
			result.Duration = time.Since(start)
			result.Timestamp = time.Now()

			hc.mutex.Lock()
			hc.results[c.Name()] = result
			hc.mutex.Unlock()
		}(check)
	}

	wg.Wait()
}

func (hc *HealthChecker) GetOverallHealth() HealthCheckStatus {
	hc.mutex.RLock()
	defer hc.mutex.RUnlock()

	if len(hc.results) == 0 {
		return HealthStatusDown
	}

	hasDown := false
	hasDegraded := false

	for _, result := range hc.results {
		switch result.Status {
		case HealthStatusDown:
			hasDown = true
		case HealthStatusDegraded:
			hasDegraded = true
		}
	}

	if hasDown {
		return HealthStatusDown
	}
	if hasDegraded {
		return HealthStatusDegraded
	}

	return HealthStatusUp
}

func (hc *HealthChecker) GetResults() map[string]HealthCheckResult {
	hc.mutex.RLock()
	defer hc.mutex.RUnlock()

	results := make(map[string]HealthCheckResult)
	for k, v := range hc.results {
		results[k] = v
	}

	return results
}

// ==================
// 7. å…·ä½“å¥åº·æ£€æŸ¥å®ç°
// ==================

// DatabaseHealthCheck æ•°æ®åº“å¥åº·æ£€æŸ¥
type DatabaseHealthCheck struct {
	name string
	dsn  string
}

func (dhc *DatabaseHealthCheck) Name() string {
	return dhc.name
}

func (dhc *DatabaseHealthCheck) Timeout() time.Duration {
	return 5 * time.Second
}

func (dhc *DatabaseHealthCheck) Check(ctx context.Context) HealthCheckResult {
	// æ¨¡æ‹Ÿæ•°æ®åº“è¿æ¥æ£€æŸ¥
	start := time.Now()

	// æ¨¡æ‹Ÿæ£€æŸ¥é€»è¾‘
	time.Sleep(time.Millisecond * time.Duration(secureRandomInt(100)))

	success := secureRandomFloat64() > 0.1 // 90%æˆåŠŸç‡

	result := HealthCheckResult{
		Name:      dhc.name,
		Duration:  time.Since(start),
		Timestamp: time.Now(),
		Details:   make(map[string]interface{}),
	}

	if success {
		result.Status = HealthStatusUp
		result.Message = "æ•°æ®åº“è¿æ¥æ­£å¸¸"
		result.Details["connection_pool_size"] = secureRandomInt(50) + 10
	} else {
		result.Status = HealthStatusDown
		result.Message = "æ•°æ®åº“è¿æ¥å¤±è´¥"
		result.Details["error"] = "connection timeout"
	}

	return result
}

// RedisHealthCheck Rediså¥åº·æ£€æŸ¥
type RedisHealthCheck struct {
	name string
	addr string
}

func (rhc *RedisHealthCheck) Name() string {
	return rhc.name
}

func (rhc *RedisHealthCheck) Timeout() time.Duration {
	return 3 * time.Second
}

func (rhc *RedisHealthCheck) Check(ctx context.Context) HealthCheckResult {
	start := time.Now()

	// æ¨¡æ‹ŸRedis PINGæ£€æŸ¥
	time.Sleep(time.Millisecond * time.Duration(secureRandomInt(50)))

	success := secureRandomFloat64() > 0.05 // 95%æˆåŠŸç‡

	result := HealthCheckResult{
		Name:      rhc.name,
		Duration:  time.Since(start),
		Timestamp: time.Now(),
		Details:   make(map[string]interface{}),
	}

	if success {
		result.Status = HealthStatusUp
		result.Message = "Redisè¿æ¥æ­£å¸¸"
		result.Details["used_memory"] = secureRandomInt(1000) + 100
		result.Details["connected_clients"] = secureRandomInt(100) + 1
	} else {
		result.Status = HealthStatusDown
		result.Message = "Redisè¿æ¥å¤±è´¥"
	}

	return result
}

// ==================
// 8. é€šçŸ¥æ¸ é“å®ç°
// ==================

// SlackNotificationChannel Slacké€šçŸ¥æ¸ é“
type SlackNotificationChannel struct {
	webhookURL string
	channel    string
}

func (snc *SlackNotificationChannel) Name() string {
	return "slack"
}

func (snc *SlackNotificationChannel) Send(alert Alert) error {
	// æ¨¡æ‹ŸSlacké€šçŸ¥å‘é€
	fmt.Printf("ğŸ“± Slacké€šçŸ¥: [%v] %s - %s\n",
		alert.Severity, alert.Name, alert.Annotations["description"])
	return nil
}

// EmailNotificationChannel é‚®ä»¶é€šçŸ¥æ¸ é“
type EmailNotificationChannel struct {
	smtpServer string
	recipients []string
}

func (enc *EmailNotificationChannel) Name() string {
	return "email"
}

func (enc *EmailNotificationChannel) Send(alert Alert) error {
	// æ¨¡æ‹Ÿé‚®ä»¶å‘é€
	fmt.Printf("ğŸ“§ é‚®ä»¶é€šçŸ¥: [%v] %s å‘é€è‡³ %v\n",
		alert.Severity, alert.Name, enc.recipients)
	return nil
}

// ==================
// 9. ä¸»æ¼”ç¤ºå‡½æ•°
// ==================

func demonstrateProductionMonitoring() {
	fmt.Println("=== Goç”Ÿäº§ç¯å¢ƒç›‘æ§ç³»ç»Ÿæ¼”ç¤º ===")

	// 1. åˆå§‹åŒ–APMç³»ç»Ÿ
	fmt.Println("\n1. åˆå§‹åŒ–APMç³»ç»Ÿ")
	config := APMConfig{
		SamplingRate:     0.1,
		MetricsInterval:  time.Second * 5,
		AlertInterval:    time.Second * 10,
		TraceRetention:   time.Hour * 24,
		MetricsRetention: time.Hour * 24 * 7,
		MaxTraces:        10000,
		MaxMetrics:       50000,
	}

	apm := NewAPMSystem(config)
	apm.Start()
	defer apm.Stop()

	// 2. æ³¨å†ŒæœåŠ¡
	fmt.Println("\n2. æ³¨å†Œå¾®æœåŠ¡")
	userService := apm.RegisterService("user-service", "v1.0.0", "instance-1")
	orderService := apm.RegisterService("order-service", "v1.2.1", "instance-1")
	paymentService := apm.RegisterService("payment-service", "v2.0.0", "instance-1")

	// 3. é…ç½®å¥åº·æ£€æŸ¥
	fmt.Println("\n3. é…ç½®å¥åº·æ£€æŸ¥")
	dbCheck := &DatabaseHealthCheck{name: "postgres", dsn: "postgres://localhost/app"}
	redisCheck := &RedisHealthCheck{name: "redis", addr: "localhost:6379"}

	apm.healthChecker.RegisterCheck(dbCheck)
	apm.healthChecker.RegisterCheck(redisCheck)

	// 4. é…ç½®å‘Šè­¦è§„åˆ™
	fmt.Println("\n4. é…ç½®å‘Šè­¦è§„åˆ™")
	highErrorRateRule := AlertRule{
		ID:        "high_error_rate",
		Name:      "é«˜é”™è¯¯ç‡å‘Šè­¦",
		Condition: GreaterThan,
		Threshold: 5.0, // 5%
		Duration:  time.Minute * 2,
		Severity:  SeverityWarning,
		Labels:    map[string]string{"service": "user-service"},
		Annotations: map[string]string{
			"description": "ç”¨æˆ·æœåŠ¡é”™è¯¯ç‡è¶…è¿‡5%",
			"runbook":     "https://runbook.example.com/high-error-rate",
		},
		Enabled: true,
	}

	highLatencyRule := AlertRule{
		ID:        "high_latency",
		Name:      "é«˜å»¶è¿Ÿå‘Šè­¦",
		Condition: GreaterThan,
		Threshold: 500.0, // 500ms
		Duration:  time.Minute * 1,
		Severity:  SeverityCritical,
		Labels:    map[string]string{"service": "payment-service"},
		Annotations: map[string]string{
			"description": "æ”¯ä»˜æœåŠ¡å»¶è¿Ÿè¶…è¿‡500ms",
		},
		Enabled: true,
	}

	apm.alerts.AddRule(highErrorRateRule)
	apm.alerts.AddRule(highLatencyRule)

	// 5. é…ç½®é€šçŸ¥æ¸ é“
	fmt.Println("\n5. é…ç½®é€šçŸ¥æ¸ é“")
	slackChannel := &SlackNotificationChannel{
		webhookURL: "https://hooks.slack.com/webhook",
		channel:    "#alerts",
	}
	emailChannel := &EmailNotificationChannel{
		smtpServer: "smtp.example.com",
		recipients: []string{"ops@example.com", "dev@example.com"},
	}

	apm.alerts.RegisterChannel("slack", slackChannel)
	apm.alerts.RegisterChannel("email", emailChannel)

	// 6. å®šä¹‰SLO
	fmt.Println("\n6. å®šä¹‰SLO")
	availabilitySLO := SLO{
		Name:        "user-service-availability",
		Description: "ç”¨æˆ·æœåŠ¡å¯ç”¨æ€§SLO",
		Target:      99.9,                // 99.9%
		TimeWindow:  time.Hour * 24 * 30, // 30å¤©
		SLIQuery:    "availability",
		Labels:      map[string]string{"service": "user-service"},
	}

	latencySLO := SLO{
		Name:        "payment-latency",
		Description: "æ”¯ä»˜æœåŠ¡å»¶è¿ŸSLO",
		Target:      95.0,               // 95% P95 < 200ms
		TimeWindow:  time.Hour * 24 * 7, // 7å¤©
		SLIQuery:    "latency_p95",
		Labels:      map[string]string{"service": "payment-service"},
	}

	apm.sloManager.DefineSLO(availabilitySLO)
	apm.sloManager.DefineSLO(latencySLO)

	// 7. æ¨¡æ‹ŸæœåŠ¡è´Ÿè½½å’Œç›‘æ§
	fmt.Println("\n7. æ¨¡æ‹ŸæœåŠ¡è´Ÿè½½")
	simulateServiceLoad(apm, userService, orderService, paymentService)

	// ç­‰å¾…ç›‘æ§æ•°æ®æ”¶é›†
	time.Sleep(10 * time.Second)

	// 8. æ‰§è¡Œå¥åº·æ£€æŸ¥
	fmt.Println("\n8. æ‰§è¡Œå¥åº·æ£€æŸ¥")
	ctx := context.Background()
	apm.healthChecker.RunChecks(ctx)

	healthResults := apm.healthChecker.GetResults()
	overallHealth := apm.healthChecker.GetOverallHealth()

	fmt.Printf("æ•´ä½“å¥åº·çŠ¶æ€: %v\n", overallHealth)
	for name, result := range healthResults {
		fmt.Printf("  %s: %v (%v)\n", name, result.Status, result.Duration)
	}

	// 9. æ£€æŸ¥å‘Šè­¦
	fmt.Println("\n9. æ£€æŸ¥å‘Šè­¦")
	metrics := apm.metrics.GetMetrics()
	apm.alerts.EvaluateRules(metrics)

	activeAlerts := apm.alerts.GetActiveAlerts()
	fmt.Printf("æ´»è·ƒå‘Šè­¦æ•°é‡: %d\n", len(activeAlerts))
	for _, alert := range activeAlerts {
		fmt.Printf("  ğŸš¨ %s: %s\n", alert.Name, alert.Annotations["description"])
	}

	// 10. æ˜¾ç¤ºè¿½è¸ªç»Ÿè®¡
	fmt.Println("\n10. åˆ†å¸ƒå¼è¿½è¸ªç»Ÿè®¡")
	traceStats := apm.traces.GetTraceStats()
	fmt.Printf("æ€»è¿½è¸ªæ•°: %v\n", traceStats["total_traces"])
	fmt.Printf("æ€»Spanæ•°: %v\n", traceStats["total_spans"])
	fmt.Printf("é”™è¯¯è¿½è¸ªæ•°: %v\n", traceStats["error_traces"])
	fmt.Printf("å¹³å‡æŒç»­æ—¶é—´: %vms\n", traceStats["avg_duration_ms"])

	// 11. SLOçŠ¶æ€æ£€æŸ¥
	fmt.Println("\n11. SLOçŠ¶æ€æ£€æŸ¥")
	apm.sloManager.CalculateErrorBudgets()
	sloStatus := apm.sloManager.GetSLOStatus()

	for name, status := range sloStatus {
		complianceStatus := "âœ… åˆè§„"
		if !status.Compliant {
			complianceStatus = "âŒ ä¸åˆè§„"
		}
		fmt.Printf("SLO %s: %.2f%% (ç›®æ ‡: %.2f%%) %s\n",
			name, status.CurrentSLI, status.Target, complianceStatus)
		fmt.Printf("  é”™è¯¯é¢„ç®—å‰©ä½™: %.2f%% (æ¶ˆè€—: %.2f%%)\n",
			status.ErrorBudget.RemainingBudget, status.ErrorBudget.ConsumedBudget)
	}

	// 12. æŒ‡æ ‡æ±‡æ€»
	fmt.Println("\n12. æ€§èƒ½æŒ‡æ ‡æ±‡æ€»")
	fmt.Printf("æ”¶é›†åˆ°çš„æŒ‡æ ‡æ•°é‡: %d\n", len(metrics))

	var requestCounts, errorRates []float64
	for name, metric := range metrics {
		if strings.Contains(name, "requests_total") {
			requestCounts = append(requestCounts, metric.Value)
		}
		if strings.Contains(name, "error_rate") {
			errorRates = append(errorRates, metric.Value)
		}
	}

	if len(requestCounts) > 0 {
		var totalRequests float64
		for _, count := range requestCounts {
			totalRequests += count
		}
		fmt.Printf("æ€»è¯·æ±‚æ•°: %.0f\n", totalRequests)
	}

	if len(errorRates) > 0 {
		var avgErrorRate float64
		for _, rate := range errorRates {
			avgErrorRate += rate
		}
		avgErrorRate /= float64(len(errorRates))
		fmt.Printf("å¹³å‡é”™è¯¯ç‡: %.2f%%\n", avgErrorRate)
	}
}

func simulateServiceLoad(apm *APMSystem, userService, orderService, paymentService *ServiceMonitor) {
	// æ¨¡æ‹Ÿç”¨æˆ·æœåŠ¡è´Ÿè½½
	go func() {
		for i := 0; i < 100; i++ {
			// æ¨¡æ‹Ÿè¯·æ±‚
			responseTime := time.Duration(secureRandomInt(200)) * time.Millisecond
			isError := secureRandomFloat64() < 0.02 // 2%é”™è¯¯ç‡

			userService.mutex.Lock()
			atomic.AddInt64(&userService.RequestCount, 1)
			if isError {
				atomic.AddInt64(&userService.ErrorCount, 1)
			}
			userService.ResponseTimes = append(userService.ResponseTimes, responseTime)
			if len(userService.ResponseTimes) > 1000 {
				userService.ResponseTimes = userService.ResponseTimes[100:]
			}
			userService.mutex.Unlock()

			// è®°å½•æŒ‡æ ‡
			apm.metrics.RecordCounter("requests_total", 1, map[string]string{
				"service": "user-service",
				"status":  fmt.Sprintf("%d", 200),
			})

			if isError {
				apm.metrics.RecordCounter("errors_total", 1, map[string]string{
					"service": "user-service",
				})
			}

			apm.metrics.RecordHistogram("response_time", float64(responseTime.Milliseconds()),
				map[string]string{"service": "user-service"},
				[]float64{10, 50, 100, 200, 500, 1000})

			time.Sleep(time.Millisecond * 50)
		}
	}()

	// æ¨¡æ‹Ÿè®¢å•æœåŠ¡è´Ÿè½½
	go func() {
		for i := 0; i < 80; i++ {
			responseTime := time.Duration(secureRandomInt(300)) * time.Millisecond
			isError := secureRandomFloat64() < 0.01 // 1%é”™è¯¯ç‡

			orderService.mutex.Lock()
			atomic.AddInt64(&orderService.RequestCount, 1)
			if isError {
				atomic.AddInt64(&orderService.ErrorCount, 1)
			}
			orderService.ResponseTimes = append(orderService.ResponseTimes, responseTime)
			if len(orderService.ResponseTimes) > 1000 {
				orderService.ResponseTimes = orderService.ResponseTimes[100:]
			}
			orderService.mutex.Unlock()

			apm.metrics.RecordCounter("requests_total", 1, map[string]string{
				"service": "order-service",
			})

			time.Sleep(time.Millisecond * 60)
		}
	}()

	// æ¨¡æ‹Ÿæ”¯ä»˜æœåŠ¡è´Ÿè½½ï¼ˆé«˜å»¶è¿Ÿåœºæ™¯ï¼‰
	go func() {
		for i := 0; i < 60; i++ {
			responseTime := time.Duration(secureRandomInt(800)+200) * time.Millisecond // 200-1000ms
			isError := secureRandomFloat64() < 0.005                                    // 0.5%é”™è¯¯ç‡

			paymentService.mutex.Lock()
			atomic.AddInt64(&paymentService.RequestCount, 1)
			if isError {
				atomic.AddInt64(&paymentService.ErrorCount, 1)
			}
			paymentService.ResponseTimes = append(paymentService.ResponseTimes, responseTime)
			if len(paymentService.ResponseTimes) > 1000 {
				paymentService.ResponseTimes = paymentService.ResponseTimes[100:]
			}
			paymentService.mutex.Unlock()

			apm.metrics.RecordGauge("payment_response_time", float64(responseTime.Milliseconds()),
				map[string]string{"service": "payment-service"})

			// åˆ›å»ºåˆ†å¸ƒå¼è¿½è¸ª
			trace := apm.traces.StartTrace("payment-service", "process_payment")

			// æ·»åŠ å­span
			span1 := apm.traces.StartSpan(trace.TraceID, trace.RootSpan.SpanID,
				"validation-service", "validate_payment")
			time.Sleep(time.Millisecond * 50)
			apm.traces.FinishSpan(span1.SpanID)

			span2 := apm.traces.StartSpan(trace.TraceID, trace.RootSpan.SpanID,
				"bank-service", "charge_card")
			time.Sleep(responseTime - time.Millisecond*50)
			apm.traces.FinishSpan(span2.SpanID)

			apm.traces.FinishSpan(trace.RootSpan.SpanID)

			time.Sleep(time.Millisecond * 100)
		}
	}()

	// æ›´æ–°SLIæ•°æ®
	go func() {
		time.Sleep(time.Second * 2)
		for i := 0; i < 50; i++ {
			// ç”¨æˆ·æœåŠ¡å¯ç”¨æ€§SLI
			goodRequests := int64(secureRandomInt(100) + 950) // 95-99.9%å¯ç”¨æ€§
			totalRequests := int64(1000)
			apm.sloManager.UpdateSLI("user-service-availability", SLIAvailability,
				goodRequests, totalRequests)

			// æ”¯ä»˜æœåŠ¡å»¶è¿ŸSLI
			fastRequests := int64(secureRandomInt(100) + 900) // 90-99%åœ¨SLAå†…
			totalPayments := int64(1000)
			apm.sloManager.UpdateSLI("payment-latency", SLILatency,
				fastRequests, totalPayments)

			time.Sleep(time.Millisecond * 100)
		}
	}()
}

func (apm *APMSystem) healthCheckLoop() {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			ctx, cancel := context.WithTimeout(context.Background(), 25*time.Second)
			apm.healthChecker.RunChecks(ctx)
			cancel()

		case <-apm.stopCh:
			return
		}
	}
}

func (apm *APMSystem) alertProcessingLoop() {
	ticker := time.NewTicker(apm.config.AlertInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			metrics := apm.metrics.GetMetrics()
			apm.alerts.EvaluateRules(metrics)

		case <-apm.stopCh:
			return
		}
	}
}

func (apm *APMSystem) traceProcessingLoop() {
	ticker := time.NewTicker(time.Minute)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			// å¤„ç†è¿½è¸ªæ•°æ®ï¼Œè®¡ç®—èšåˆæŒ‡æ ‡ç­‰
			// å®é™…å®ç°ä¼šæ›´å¤æ‚

		case <-apm.stopCh:
			return
		}
	}
}

func main() {
	demonstrateProductionMonitoring()

	fmt.Println("\n=== Goç”Ÿäº§ç¯å¢ƒç›‘æ§ç³»ç»Ÿæ¼”ç¤ºå®Œæˆ ===")
	fmt.Println("\nå­¦ä¹ è¦ç‚¹æ€»ç»“:")
	fmt.Println("1. APMç³»ç»Ÿï¼šå…¨æ–¹ä½åº”ç”¨æ€§èƒ½ç›‘æ§æ¶æ„")
	fmt.Println("2. åˆ†å¸ƒå¼è¿½è¸ªï¼šè·¨æœåŠ¡çš„è¯·æ±‚é“¾è·¯è¿½è¸ª")
	fmt.Println("3. SLI/SLOç®¡ç†ï¼šæœåŠ¡ç­‰çº§ç›®æ ‡å’Œé”™è¯¯é¢„ç®—")
	fmt.Println("4. å¥åº·æ£€æŸ¥ï¼šè‡ªåŠ¨åŒ–æœåŠ¡å¥åº·çŠ¶æ€ç›‘æ§")
	fmt.Println("5. å‘Šè­¦ç³»ç»Ÿï¼šæ™ºèƒ½åŒ–çš„é—®é¢˜å‘ç°å’Œé€šçŸ¥")
	fmt.Println("6. æŒ‡æ ‡èšåˆï¼šå¤šç»´åº¦æ€§èƒ½æ•°æ®æ”¶é›†å’Œåˆ†æ")
	fmt.Println("7. ç”Ÿäº§ç›‘æ§ï¼šä¼ä¸šçº§ç›‘æ§ç³»ç»Ÿæœ€ä½³å®è·µ")

	fmt.Println("\nä¼ä¸šçº§ç‰¹æ€§:")
	fmt.Println("- å¾®æœåŠ¡æ¶æ„ä¸‹çš„å…¨é“¾è·¯ç›‘æ§")
	fmt.Println("- åŸºäºSLOçš„å¯é æ€§å·¥ç¨‹")
	fmt.Println("- è‡ªåŠ¨åŒ–çš„æ•…éšœæ£€æµ‹å’Œæ¢å¤")
	fmt.Println("- å¤šæ¸ é“å‘Šè­¦å’Œé€šçŸ¥ç³»ç»Ÿ")
	fmt.Println("- å®¹é‡è§„åˆ’å’Œæ€§èƒ½é¢„æµ‹")
	fmt.Println("- å®æ—¶ä»ªè¡¨æ¿å’Œå¯è§†åŒ–")
}

/*
=== ç»ƒä¹ é¢˜ ===

1. å¾®æœåŠ¡ç›‘æ§ï¼š
   - å®ç°æœåŠ¡ç½‘æ ¼ç›‘æ§é›†æˆ
   - æ·»åŠ ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§
   - åˆ›å»ºæœåŠ¡ä¾èµ–å…³ç³»å›¾
   - å®ç°è°ƒç”¨é“¾è·¯åˆ†æ

2. å‘Šè­¦ä¼˜åŒ–ï¼š
   - å®ç°åŠ¨æ€é˜ˆå€¼è°ƒæ•´
   - æ·»åŠ å‘Šè­¦èšåˆå’Œå»é‡
   - åˆ›å»ºå‘Šè­¦å‡çº§æœºåˆ¶
   - å®ç°æ™ºèƒ½å‘Šè­¦é™é»˜

3. SLOå·¥ç¨‹ï¼š
   - å®ç°é”™è¯¯é¢„ç®—ç­–ç•¥
   - æ·»åŠ SLOåˆè§„æ€§æŠ¥å‘Š
   - åˆ›å»ºSLOé©±åŠ¨çš„å‘å¸ƒç­–ç•¥
   - å®ç°å®¹é‡è§„åˆ’å·¥å…·

4. ç”Ÿäº§éƒ¨ç½²ï¼š
   - é›†æˆKubernetesç›‘æ§
   - å®ç°å¤šäº‘ç¯å¢ƒç›‘æ§
   - æ·»åŠ æˆæœ¬ç›‘æ§åŠŸèƒ½
   - åˆ›å»ºæ€§èƒ½åŸºçº¿ç®¡ç†

è¿è¡Œå‘½ä»¤ï¼š
go run main.go

é›†æˆå·¥å…·ï¼š
- Prometheus + Grafana
- Jaeger åˆ†å¸ƒå¼è¿½è¸ª
- Alertmanager å‘Šè­¦ç®¡ç†
- ELK Stack æ—¥å¿—èšåˆ

é‡è¦æ¦‚å¿µï¼š
- APM: Application Performance Monitoring
- SLI: Service Level Indicators
- SLO: Service Level Objectives
- Error Budget: é”™è¯¯é¢„ç®—ç®¡ç†
- Distributed Tracing: åˆ†å¸ƒå¼è¿½è¸ª
- Health Check: å¥åº·æ£€æŸ¥
*/
